"""
LLM í´ë¼ì´ì–¸íŠ¸ í†µí•© ë˜í¼

ì´ ëª¨ë“ˆì€ OpenAIì™€ Anthropic APIë¥¼ í†µí•©í•˜ì—¬ ê´€ë¦¬í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
- LangChainì˜ with_structured_output() ë©”ì†Œë“œ í™œìš©
- ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
- í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
- Fallback ì „ëµ êµ¬í˜„
"""

import os
import time
import logging
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, Optional, Type, Union, List, Tuple
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum

from pydantic import BaseModel, Field
import openai
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.runnables import RunnablePassthrough
from langchain_core.exceptions import LangChainException

from ..models.job_posting import UserInput, JobPostingTemplate
from ..exceptions import LLMError, ValidationError

logger = logging.getLogger(__name__)

# ğŸ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ .env íŒŒì¼ ë¡œë“œ
project_root = Path(__file__).parent.parent.parent  # src/components/llm_client.py -> í”„ë¡œì íŠ¸ ë£¨íŠ¸
env_path = project_root / ".env"
load_result = load_dotenv(env_path, override=True)
logger.info(f"ğŸ” .env íŒŒì¼ ë¡œë“œ - ê²½ë¡œ: {env_path}, ì„±ê³µ: {load_result}, íŒŒì¼ ì¡´ì¬: {env_path.exists()}")

class LLMProvider(str, Enum):
    """LLM ì œê³µì—…ì²´ ì—´ê±°í˜•"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"


class ModelType(str, Enum):
    """ëª¨ë¸ íƒ€ì… ì—´ê±°í˜•"""
    # OpenAI ëª¨ë¸
    GPT_O3_MINI = "gpt-o3-mini"
    GPT_5_MINI = "gpt-5-mini"
    GPT_5 = "gpt-5"
    
    # Anthropic ëª¨ë¸
    CLAUDE_4_1_OPUS = "claude-opus-4-1-20250805"
    CLAUDE_4_SONNET = "claude-sonnet-4-20250514" 
    CLAUDE_3_5_HAIKU = "claude-3-5-haiku-latest"


@dataclass
class LLMUsageStats:
    """LLM ì‚¬ìš©ëŸ‰ í†µê³„"""
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    request_count: int = 0
    total_cost: float = 0.0
    generation_time: float = 0.0


@dataclass
class LLMConfig:
    """LLM ì„¤ì •"""
    provider: LLMProvider
    model: ModelType
    temperature: float = 0.1
    max_tokens: int = 6000  # í† í° ì œí•œ í™•ëŒ€
    max_retries: int = 3
    timeout: int = 60
    api_key: Optional[str] = None
    
    def __post_init__(self):
        """API í‚¤ í™˜ê²½ë³€ìˆ˜ì—ì„œ ìë™ ì„¤ì •"""
        if not self.api_key:
            if self.provider == LLMProvider.OPENAI:
                self.api_key = os.getenv("OPENAI_API_KEY")
            elif self.provider == LLMProvider.ANTHROPIC:
                self.api_key = os.getenv("ANTHROPIC_API_KEY")


class BaseLLMClient(ABC):
    """ê¸°ë³¸ LLM í´ë¼ì´ì–¸íŠ¸ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.stats = LLMUsageStats()
        self.client = None
        self._initialize_client()
    
    @abstractmethod
    def _initialize_client(self):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¶”ìƒ ë©”ì„œë“œ"""
        pass
    
    @abstractmethod
    def generate_structured_output(self, 
                                 system_prompt: str, 
                                 user_prompt: str, 
                                 output_schema: Type[BaseModel],
                                 **kwargs) -> BaseModel:
        """êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„± ì¶”ìƒ ë©”ì„œë“œ"""
        pass
    
    def get_stats(self) -> LLMUsageStats:
        """ì‚¬ìš©ëŸ‰ í†µê³„ ë°˜í™˜"""
        return self.stats


class OpenAIClient(BaseLLMClient):
    """OpenAI LLM í´ë¼ì´ì–¸íŠ¸"""
    
    def _initialize_client(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            if not self.config.api_key:
                raise LLMError("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                
            self.client = ChatOpenAI(
                model=self.config.model.value,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens, # type: ignore
                max_retries=self.config.max_retries,
                timeout=self.config.timeout,
                api_key=self.config.api_key
            )
            
            logger.info(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {self.config.model.value}")
            
        except Exception as e:
            logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise LLMError(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    
    def generate_structured_output(self, 
                                 system_prompt: str, 
                                 user_prompt: str, 
                                 output_schema: Type[BaseModel],
                                 **kwargs) -> BaseModel:
        """êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±"""
        start_time = time.time()
        
        try:
            logger.info(f"OpenAI êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„± ì‹œì‘: {output_schema.__name__}")
            
            # with_structured_outputì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±
            structured_llm = self.client.with_structured_output(output_schema)
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            # LLM í˜¸ì¶œ
            result = structured_llm.invoke(messages)

            # íƒ€ì… ë³€í™˜: ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° Pydantic ëª¨ë¸ë¡œ ë³€í™˜
            if isinstance(result, dict):
                result = output_schema(**result)
            elif not isinstance(result, BaseModel):
                raise LLMError(f"ì˜ˆìƒì¹˜ ëª»í•œ ë°˜í™˜ íƒ€ì…: {type(result)}")

            # í†µê³„ ì—…ë°ì´íŠ¸
            generation_time = time.time() - start_time
            self.stats.request_count += 1
            self.stats.generation_time += generation_time

            # í† í° ì‚¬ìš©ëŸ‰ì€ ì‹¤ì œ êµ¬í˜„ì—ì„œ response.usage_metadata ë“±ì„ í†µí•´ ì¶”ì¶œí•´ì•¼ í•¨
            # í˜„ì¬ëŠ” ì„ì‹œë¡œ ì¶”ì •ê°’ ì‚¬ìš©
            estimated_prompt_tokens = len(system_prompt + user_prompt) // 4
            estimated_completion_tokens = 500  # ì„ì‹œ ì¶”ì •ê°’

            self.stats.prompt_tokens += estimated_prompt_tokens
            self.stats.completion_tokens += estimated_completion_tokens
            self.stats.total_tokens += estimated_prompt_tokens + estimated_completion_tokens

            logger.info(f"OpenAI êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„± ì™„ë£Œ ({generation_time:.2f}ì´ˆ)")

            return result
            
        except Exception as e:
            logger.error(f"OpenAI êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise LLMError(f"OpenAI êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„± ì‹¤íŒ¨: {str(e)}")


class AnthropicClient(BaseLLMClient):
    """Anthropic LLM í´ë¼ì´ì–¸íŠ¸"""
    
    def _initialize_client(self):
        """Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            if not self.config.api_key:
                raise LLMError("Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                
            self.client = ChatAnthropic(
                model=self.config.model.value, # type: ignore
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens, # type: ignore
                max_retries=self.config.max_retries,
                timeout=self.config.timeout,
                api_key=self.config.api_key
            ) # type: ignore
            
            logger.info(f"Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {self.config.model.value}")
            
        except Exception as e:
            logger.error(f"Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            raise LLMError(f"Anthropic í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    
    def generate_structured_output(self, 
                                 system_prompt: str, 
                                 user_prompt: str, 
                                 output_schema: Type[BaseModel],
                                 **kwargs) -> BaseModel:
        """êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±"""
        start_time = time.time()
        
        try:
            logger.info(f"Anthropic êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„± ì‹œì‘: {output_schema.__name__}")
            
            # with_structured_outputì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±
            structured_llm = self.client.with_structured_output(output_schema)
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_prompt)
            ]
            
            # LLM í˜¸ì¶œ
            result = structured_llm.invoke(messages)

            # íƒ€ì… ë³€í™˜: ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° Pydantic ëª¨ë¸ë¡œ ë³€í™˜
            if isinstance(result, dict):
                result = output_schema(**result)
            elif not isinstance(result, BaseModel):
                raise LLMError(f"ì˜ˆìƒì¹˜ ëª»í•œ ë°˜í™˜ íƒ€ì…: {type(result)}")

            # í†µê³„ ì—…ë°ì´íŠ¸
            generation_time = time.time() - start_time
            self.stats.request_count += 1
            self.stats.generation_time += generation_time

            # í† í° ì‚¬ìš©ëŸ‰ì€ ì‹¤ì œ êµ¬í˜„ì—ì„œ response.usage_metadata ë“±ì„ í†µí•´ ì¶”ì¶œí•´ì•¼ í•¨
            # í˜„ì¬ëŠ” ì„ì‹œë¡œ ì¶”ì •ê°’ ì‚¬ìš©
            estimated_prompt_tokens = len(system_prompt + user_prompt) // 4
            estimated_completion_tokens = 500  # ì„ì‹œ ì¶”ì •ê°’

            self.stats.prompt_tokens += estimated_prompt_tokens
            self.stats.completion_tokens += estimated_completion_tokens
            self.stats.total_tokens += estimated_prompt_tokens + estimated_completion_tokens

            logger.info(f"Anthropic êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„± ì™„ë£Œ ({generation_time:.2f}ì´ˆ)")

            return result
            
        except Exception as e:
            logger.error(f"Anthropic êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise LLMError(f"Anthropic êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„± ì‹¤íŒ¨: {str(e)}")


class LLMClientManager:
    """
    í†µí•© LLM í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬ì
    
    ì—¬ëŸ¬ LLM ì œê³µì—…ì²´ë¥¼ ê´€ë¦¬í•˜ê³  Fallback ì „ëµì„ êµ¬í˜„í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.clients: Dict[str, BaseLLMClient] = {}
        self.primary_client: Optional[BaseLLMClient] = None
        self.fallback_clients: List[BaseLLMClient] = []
        self._initialize_default_clients()
    
    def _initialize_default_clients(self):
        """ê¸°ë³¸ í´ë¼ì´ì–¸íŠ¸ë“¤ ì´ˆê¸°í™”"""
        try:
            # ğŸ” ë””ë²„ê¹…: í™˜ê²½ë³€ìˆ˜ ìƒíƒœ í™•ì¸
            openai_key = os.getenv("OPENAI_API_KEY")
            anthropic_key = os.getenv("ANTHROPIC_API_KEY")
            logger.info(f"ğŸ” í™˜ê²½ë³€ìˆ˜ í™•ì¸ - OPENAI_API_KEY: {'ìˆìŒ' if openai_key else 'ì—†ìŒ'}, ANTHROPIC_API_KEY: {'ìˆìŒ' if anthropic_key else 'ì—†ìŒ'}")
            
            # OpenAI í´ë¼ì´ì–¸íŠ¸ (Primary)
            if openai_key:
                openai_config = LLMConfig(
                    provider=LLMProvider.OPENAI,
                    model=ModelType.GPT_5_MINI,
                    temperature=0.1,
                    max_tokens=6000
                )
                openai_client = OpenAIClient(openai_config)
                self.clients["openai"] = openai_client
                self.primary_client = openai_client
                logger.info("OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ Primaryë¡œ ì„¤ì •ë¨")
            
            # Anthropic í´ë¼ì´ì–¸íŠ¸ (Fallback)
            if anthropic_key:
                anthropic_config = LLMConfig(
                    provider=LLMProvider.ANTHROPIC,
                    model=ModelType.CLAUDE_4_SONNET,
                    temperature=0.1,
                    max_tokens=6000
                )
                anthropic_client = AnthropicClient(anthropic_config)
                self.clients["anthropic"] = anthropic_client
                self.fallback_clients.append(anthropic_client)
                logger.info("Anthropic í´ë¼ì´ì–¸íŠ¸ê°€ Fallbackìœ¼ë¡œ ì„¤ì •ë¨")
            
            # ğŸ” ë””ë²„ê¹…: ìµœì¢… í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ í™•ì¸
            logger.info(f"ğŸ” í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ - Primary: {'ìˆìŒ' if self.primary_client else 'ì—†ìŒ'}, Fallback: {len(self.fallback_clients)}ê°œ")
                
        except Exception as e:
            logger.warning(f"ì¼ë¶€ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    
    def add_client(self, name: str, client: BaseLLMClient, is_primary: bool = False):
        """í´ë¼ì´ì–¸íŠ¸ ì¶”ê°€"""
        self.clients[name] = client
        
        if is_primary:
            self.primary_client = client
        else:
            self.fallback_clients.append(client)
        
        logger.info(f"í´ë¼ì´ì–¸íŠ¸ ì¶”ê°€: {name} ({'Primary' if is_primary else 'Fallback'})")
    
    def generate_structured_output(self, 
                                 system_prompt: str, 
                                 user_prompt: str, 
                                 output_schema: Type[BaseModel],
                                 **kwargs) -> Tuple[BaseModel, str]:
        """êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„± (Fallback ì „ëµ í¬í•¨)"""
        
        # ğŸ” ë””ë²„ê¹…: ë©”ì†Œë“œ í˜¸ì¶œ í™•ì¸
        logger.info(f"ğŸ” generate_structured_output í˜¸ì¶œë¨ - Primary: {'ìˆìŒ' if self.primary_client else 'ì—†ìŒ'}, Fallback: {len(self.fallback_clients)}ê°œ")
        
        # Primary í´ë¼ì´ì–¸íŠ¸ ì‹œë„
        if self.primary_client:
            try:
                logger.info("Primary í´ë¼ì´ì–¸íŠ¸ë¡œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„± ì‹œë„")
                return self.primary_client.generate_structured_output(
                    system_prompt, user_prompt, output_schema, **kwargs
                ), self.primary_client.config.model.value
            except Exception as e:
                logger.warning(f"Primary í´ë¼ì´ì–¸íŠ¸ ì‹¤íŒ¨: {str(e)}")
        
        # Fallback í´ë¼ì´ì–¸íŠ¸ë“¤ ìˆœì°¨ ì‹œë„
        for i, client in enumerate(self.fallback_clients):
            try:
                logger.info(f"Fallback í´ë¼ì´ì–¸íŠ¸ {i+1} ì‹œë„")
                return client.generate_structured_output(
                    system_prompt, user_prompt, output_schema, **kwargs
                ), self.fallback_clients[i].config.model.value
            except Exception as e:
                logger.warning(f"Fallback í´ë¼ì´ì–¸íŠ¸ {i+1} ì‹¤íŒ¨: {str(e)}")
                continue
        
        # ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ ì‹¤íŒ¨
        raise LLMError("ëª¨ë“  LLM í´ë¼ì´ì–¸íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
    
    def get_client_stats(self) -> Dict[str, LLMUsageStats]:
        """ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ì˜ ì‚¬ìš©ëŸ‰ í†µê³„ ë°˜í™˜"""
        return {name: client.get_stats() for name, client in self.clients.items()}
    
    def get_available_clients(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ í´ë¼ì´ì–¸íŠ¸ ëª©ë¡ ë°˜í™˜"""
        return list(self.clients.keys())


# ì „ì—­ LLM í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤)
_llm_manager_instance = None

def get_llm_manager() -> LLMClientManager:
    """LLM í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _llm_manager_instance
    if _llm_manager_instance is None:
        _llm_manager_instance = LLMClientManager()
    return _llm_manager_instance
